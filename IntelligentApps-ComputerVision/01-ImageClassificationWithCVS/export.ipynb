{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Part 2 - Export a model\n\n\nIn this section of the lab you will export the trained model and learn how to use it in your application. \nCustom Vision Service allows classifiers to be exported to run offline. You can embed your exported classifier into an application and run it locally on a device for real-time classification.\n\nCustom Vision Service supports the following exports:\n\n- Tensorflow for Android.\n- CoreML for iOS11.\n- ONNX for Windows ML.\n- A Windows or Linux container. The container includes a Tensorflow model and service code to use the Custom Vision Service API.\n\nCustom Vision Service only exports compact domains. The models generated by compact domains are optimized for the constraints of real-time classification on low powered devices. Classifiers built with a compact domain may be slightly less accurate than a standard domain with the same amount of training data.\n\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Before you start\nInstall `wget` package that will be used later in the lab"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Install Custom Vision Service SDK  in the current Jupyter kernel\nimport sys\n!{sys.executable} -m pip install wget",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Retrieve your training and prediction keys."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Create a `trainer` object using your keys"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\nfrom azure.cognitiveservices.vision.customvision.training.models import ImageUrlCreateEntry\n\nENDPOINT = \"https://southcentralus.api.cognitive.microsoft.com\"\n\n# update keys\ntraining_key = ''\nprediction_key = ''\n\ntrainer = CustomVisionTrainingClient(training_key, endpoint=ENDPOINT)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Retrain with a compact domain\nIf your classifier was not trained with the compact domain (which is the case for the classifier trained during the first stage of the lab) you need to retrain it.\n\nWhen you first created your project, it was configured with the `General` domain. A domain represents a pre-trained deep neural network that will be used as a base of your custom image classifier. Each domain optimizes the classifier for specific types of images. `General` is optimized for a broad range of classification tasks. If none of the other domains (`Food`, `Landmarks`, `Retail`, `Adult`) are appropriate, or you are unsure of which domain to choose, select the `General` domain.\n\nThe compact domains are optimized for the constraints of real-time classification on low powered and mobile devices. The models generated by compact domains can be exported to run locally."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Print the list of available domains\nfor domain in trainer.get_domains():\n print(domain)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "First, find the ID of the `General (compact)` domain."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Find the ID of General (compact) domain\ndomain_name = 'General (compact)'\nmodeltype = 'Classification'\ndomain_id = None\nfor domain in trainer.get_domains():\n    if domain.name == domain_name:\n        if domain.type == modeltype:\n            domain_id = domain.id\n            print(\"Found domain: {} {} type with ID: {}\".format(domain_name,modeltype,domain_id))\n            break\n\nif domain_id == None:\n    print(\"Could not find domain: {}\".format(domain_name))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next find your project's ID using your project's name "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Find you project's ID\n# Make sure project id same as previous training notebook\nproject_name = 'AerialClassifier'\n\nproject_id = None\nproject = None\nfor prj in trainer.get_projects():\n    if prj.name == project_name:\n        project_id = prj.id\n        project = prj\n        print(\"Found project: {0} known as {1}\".format(project_id, project_name))\n        break\n        \nif project_id == None:\n    print(\"Could not find your project\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "And finally, change the domain of your project."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Change the project's domain\nproject.settings.domain_id = domain_id\nproject = trainer.update_project(project_id, project)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Retrain\n\nYou will use the same helper function you used in Part 1 of the lab to retrain the model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import time\n\ndef train(training_key, project_id):\n    trainer = CustomVisionTrainingClient(training_key, endpoint=ENDPOINT)\n    print(\"Starting training...\")\n    try:\n        iteration = trainer.train_project(project_id)\n        while (iteration.status != \"Completed\"):\n            time.sleep(5)\n            iteration = trainer.get_iteration(project_id, iteration.id)\n            print (\"Training status: \" + iteration.status)      \n        # The iteration is now trained. Make it the default project endpoint\n        print(\"Training completed\")\n        trainer.update_iteration(project_id, iteration.id, is_default=True)\n    except:\n        print(\"No need to retrain. Retrieving default iteration\")\n        for iteration in trainer.get_iterations(project_id):\n            if iteration.is_default:\n                break\n\n    return iteration.id",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Start training\niteration_id = train(training_key, project_id)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Export the iteration\nExporting a model is a two-step process. First you must request an export. It is an asynchronous call. After you requested the export, you should periodically check the status of the request and when the export package is ready you can download it using the returned URI.\n\n\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Request the export and wait for till it is fulfilled."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Requesting export for Iteration ID: {0}\".format(iteration_id))\n\nplatform = 'TensorFlow'\nflavor = 'Linux'\n\nexport = trainer.export_iteration(project_id, iteration_id, platform, flavor)\nwhile (export.status != 'Done'):\n   print (\"Export status: \" + export.status)\n   time.sleep(5)\n   export = trainer.get_exports(project_id, iteration_id)[0]\n\nprint(\"Export package ready. Download URI: {}\", export.download_uri)    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now you can download the package."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import wget\nimport os\n\ndownload_filename = 'aerialclassifier.zip'\n\nprint(\"Downloading from: {0}\".format(export.download_uri))\nwget.download(export.download_uri, download_filename)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The package has been exported as a zip file."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%sh\nunzip aerialclassifier.zip\nls -l ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It contains two files: `model.pb` and `labels.txt`. These files represent the trained model and the classification labels. The model has been exported in TensorFlow protocol buffers format. You will need TensorFlow runtime to execute model."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Run TensorFlow model "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Install TensorFlow and other dependencies required to load and run your model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import sys\n!{sys.executable} -m pip install tensorflow==1.7\n!{sys.executable} -m pip install pillow\n!{sys.executable} -m pip install numpy",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can now load the saved model to a TensorFlow computational graph."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\nimport os\n\nfilename = 'model.pb'\nlabels_filename = 'labels.txt'\n\ngraph_def = tf.GraphDef()\nlabels = []\n\n# Import the TF graph\nwith tf.gfile.FastGFile(filename, 'rb') as f:\n    graph_def.ParseFromString(f.read())\n    tf.import_graph_def(graph_def, name='')\n\n# Create a list of labels.\nwith open(labels_filename, 'rt') as lf:\n    for l in lf:\n        labels.append(l.strip())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Prepare an image for prediction\n\nThere are a few steps for preparing the image so that it's the right shape for prediction. The exported model requires images to be BGR format of size (227, 227). Our testing images are RGB of size (224, 224) \n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from PIL import Image\nimport numpy as np\n\n# Load from a file\nimageFile = \"test_images/developed-1.png\"\nimage = Image.open(imageFile)\nprint(image\n     )\n# Resize \nimage = image.resize((224, 224), resample = Image.BILINEAR)\nprint(image)\n\n# Convert to numpy array - tensor\nimage_tensor = np.asarray(image)\n\n# Convert RGB -> BGR \nr,g,b = np.array(image_tensor).T\nimage_tensor = np.array([b,g,r]).transpose()\n\nprint(\"Numpy array mode=BGR shape={}\".format(image_tensor.shape))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Predict an image\nOnce the image is prepared as a tensor of the required shape and format, we can send it through the model for a prediction."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# These names are part of the model and cannot be changed.\noutput_layer = 'loss:0'\ninput_node = 'Placeholder:0'\n\nwith tf.Session() as sess:\n    prob_tensor = sess.graph.get_tensor_by_name(output_layer)\n    predictions, = sess.run(prob_tensor, {input_node: [image_tensor] })\n    \nprint(predictions)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### View the results\nThe result of running the image tensor through the model is a vector of probabilities of the image belonging to each of the classes. To make the result more readable you will map it back to the labels."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Print the highest probability label\nhighest_probability_index = np.argmax(predictions)\nprint('Classified as: ' + labels[highest_probability_index])\nprint()\n\n# Or you can print out all of the results mapping labels to probabilities.\nlabel_index = 0\nfor p in predictions:\n    truncated_probablity = np.float64(np.round(p,8))\n    print (labels[label_index], truncated_probablity)\n    label_index += 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Summary\n\nIn this lab, you exported the trained model and then used it in Python script for inference - a.k.a predictions. As you have seen, you can embedded your model in an arbitrary application. TensorFlow APIs are available in several languages besides Python, including JavaScript, C++, Java, Go, and Swift. In addition, you can use TensorFlow Serving and Azure Machine Learning service for model deployment in production."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}